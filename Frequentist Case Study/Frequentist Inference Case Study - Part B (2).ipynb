{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequentist Inference Case Study - Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Part B of the Frequentist inference case study! The purpose of this case study is to help you apply the concepts associated with Frequentist inference in Python. In particular, you'll practice writing Python code to apply the following statistical concepts: \n",
    "* the _z_-statistic\n",
    "* the _t_-statistic\n",
    "* the difference and relationship between the two\n",
    "* the Central Limit Theorem, including its assumptions and consequences\n",
    "* how to estimate the population mean and standard deviation from a sample\n",
    "* the concept of a sampling distribution of a test statistic, particularly for the mean\n",
    "* how to combine these concepts to calculate a confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we used only data from a known normal distribution. **You'll now tackle real data, rather than simulated data, and answer some relevant real-world business problems using the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospital medical charges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that a hospital has hired you as their data scientist. An administrator is working on the hospital's business operations plan and needs you to help them answer some business questions. \n",
    "\n",
    "In this assignment notebook, you're going to use frequentist statistical inference on a data sample to answer the questions:\n",
    "* has the hospital's revenue stream fallen below a key threshold?\n",
    "* are patients with insurance really charged different amounts than those without?\n",
    "\n",
    "Answering that last question with a frequentist approach makes some assumptions, and requires some knowledge, about the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use some data on medical charges obtained from [Kaggle](https://www.kaggle.com/easonlai/sample-insurance-claim-prediction-dataset). \n",
    "\n",
    "For the purposes of this exercise, assume the observations are the result of random sampling from our single hospital. Recall that in the previous assignment, we introduced the Central Limit Theorem (CLT), and its consequence that the distributions of sample statistics approach a normal distribution as $n$ increases. The amazing thing about this is that it applies to the sampling distributions of statistics that have been calculated from even highly non-normal distributions of data! Recall, also, that hypothesis testing is very much based on making inferences about such sample statistics. You're going to rely heavily on the CLT to apply frequentist (parametric) tests to answer the questions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "from numpy.random import seed\n",
    "medical = pd.read_csv('data/insurance2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1338, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "      <th>insuranceclaim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex     bmi  children  smoker  region      charges  insuranceclaim\n",
       "0   19    0  27.900         0       1       3  16884.92400               1\n",
       "1   18    1  33.770         1       0       2   1725.55230               1\n",
       "2   28    1  33.000         3       0       2   4449.46200               0\n",
       "3   33    1  22.705         0       0       1  21984.47061               0\n",
       "4   32    1  28.880         0       0       1   3866.85520               1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q1:__ Plot the histogram of charges and calculate the mean and standard deviation. Comment on the appropriateness of these statistics for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = medical['charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([536., 398., 129.,  86.,  35.,  59.,  57.,  32.,   2.,   4.]),\n",
       " array([ 1121.8739  ,  7386.729311, 13651.584722, 19916.440133,\n",
       "        26181.295544, 32446.150955, 38711.006366, 44975.861777,\n",
       "        51240.717188, 57505.572599, 63770.42801 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the histogram, we can see the values are heavily to one side not evenly distributed. The std will be large. The mean and std of the sample won't be appropriate by themselves to give a good understanding of this data. The mean and std of the full population are unknown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13270.422265141257"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbar = x.mean()\n",
    "xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12110.011236693994"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = x.std()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q2:__ The administrator is concerned that the actual average charge has fallen below 12,000, threatening the hospital's operational model. On the assumption that these data represent a random sample of charges, how would you justify that these data allow you to answer that question? And what would be the most appropriate frequentist test, of the ones discussed so far, to apply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ The size of this sample is 1338 which is large and the sample is also random. The most appropriate frequentist test, of the ones discussed so far, would be the t-test. We'll want to determine the confidence interval and because the population std is unknonw, the t distribution will be used as the critical value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q3:__ Given the nature of the administrator's concern, what is the appropriate confidence interval in this case? A ***one-sided*** or ***two-sided*** interval? (Refresh your understanding of this concept on p. 399 of the *AoS*). Calculate the critical value and the relevant 95% confidence interval for the mean, and comment on whether the administrator should be concerned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ A one-sided interval makes sense because the admin does not care about charges being too high; too low of an average charge is the concern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = x.size\n",
    "dof = n - 1\n",
    "#for one-sided we will use 95%, not 97.5%: \n",
    "prob = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the critical value\n",
    "tscore = t(dof).ppf(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12725.48718381623, 13815.357346466282]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the confidence interval\n",
    "SE = s / n**.5\n",
    "MoE = tscore * SE\n",
    "confidence_interval = [xbar - MoE, xbar + MoE]\n",
    "confidence_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-sided confidence interval which would actually interest us starts at the sample mean minus the margin of error but ends at infinity. confidence_interval[0] to infinity would be a 97.5% confidence interval though. Let's do another so we end up with only 95% for our one-sided test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12793.56630478045, inf]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the relevant 95% confidence interval\n",
    "prob = .95 - ((1-.95)/2)\n",
    "tscore = t(dof).ppf(prob)\n",
    "MoE = tscore * SE\n",
    "confidence_interval = [xbar - MoE, np.inf]\n",
    "confidence_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The admin's concern is if the population mean < 12000. That value is not within either of our confidence intervals. They should not be concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The administrator then wants to know whether people with insurance really are charged a different amount to those without.\n",
    "\n",
    "__Q4:__ State the null and alternative hypothesis here. Use the _t_-test for the difference between means, where the pooled standard deviation of the two groups is given by:\n",
    "\\begin{equation}\n",
    "s_p = \\sqrt{\\frac{(n_0 - 1)s^2_0 + (n_1 - 1)s^2_1}{n_0 + n_1 - 2}}\n",
    "\\end{equation}\n",
    "\n",
    "and the *t*-test statistic is then given by:\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\bar{x}_0 - \\bar{x}_1}{s_p \\sqrt{1/n_0 + 1/n_1}}.\n",
    "\\end{equation}\n",
    "\n",
    "(If you need some reminding of the general definition of ***t-statistic***, check out the definition on p. 404 of *AoS*). \n",
    "\n",
    "What assumption about the variances of the two groups are we making here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ \n",
    "Our null is that insured people *are* charged differently compared to those without. The admin asked if they \"really are\" so we are only confirming the status queue if they are charged differently. We are willing to assume they are, until proven otherwise. Therefore, this will be our null. \n",
    "\n",
    "The assumption about the variances of the two groups we are making here is that they are the same. This is because it would otherwise be violating the assumption of homogeneity of variances and we would use a different method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q5:__ Perform this hypothesis test both manually, using the above formulae, and then using the appropriate function from [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html#statistical-tests) (hint, you're looking for a function to perform a _t_-test on two independent samples). For the manual approach, calculate the value of the test statistic and then its probability (the p-value). Verify you get the same results from both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>insuranceclaim</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   insuranceclaim      charges\n",
       "0               1  16884.92400\n",
       "1               1   1725.55230\n",
       "2               0   4449.46200\n",
       "3               0  21984.47061\n",
       "4               1   3866.85520"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = medical[['insuranceclaim', 'charges']]\n",
    "xs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "insuranceclaim\n",
       "0    555\n",
       "1    783\n",
       "Name: charges, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = xs.groupby('insuranceclaim')['charges'].count()\n",
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "insuranceclaim\n",
       "0     6446.510127\n",
       "1    14045.928419\n",
       "Name: charges, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = xs.groupby('insuranceclaim')['charges'].std()\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "insuranceclaim\n",
       "0     8821.421892\n",
       "1    16423.928277\n",
       "Name: charges, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbars = xs.groupby('insuranceclaim')['charges'].mean()\n",
    "xbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11520.034268775256"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the formula to get pooled deviation\n",
    "wv = (ns[0]-1) * stds[0]**2 + (ns[1]-1) * stds[1]**2\n",
    "dof = ns[0] + ns[1] - 2\n",
    "std_pooled = (wv/dof)**.5\n",
    "std_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.89329903087671"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the formula to get pooled tscore\n",
    "tstat = (xbars[0]-xbars[1]) / ( std_pooled * (1/ns[0] + 1/ns[1])**.5 )\n",
    "tstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6459949688112576"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate tvalue the way we did in part A: \n",
    "prob = .95\n",
    "alpha = 1 - prob\n",
    "#\n",
    "tvalue = t(dof).ppf(prob)\n",
    "tvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our null is that the population means will not be the same, the distribution of our test statistic under the assumption that it is true should be the two tails, excluding the middle area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.461230231620972e-31"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue = 2 * t(dof).cdf(tstat)\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ttest_ind in module scipy.stats.stats:\n",
      "\n",
      "ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate', alternative='two-sided')\n",
      "    Calculate the T-test for the means of *two independent* samples of scores.\n",
      "    \n",
      "    This is a two-sided test for the null hypothesis that 2 independent samples\n",
      "    have identical average (expected) values. This test assumes that the\n",
      "    populations have identical variances by default.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a, b : array_like\n",
      "        The arrays must have the same shape, except in the dimension\n",
      "        corresponding to `axis` (the first, by default).\n",
      "    axis : int or None, optional\n",
      "        Axis along which to compute test. If None, compute over the whole\n",
      "        arrays, `a`, and `b`.\n",
      "    equal_var : bool, optional\n",
      "        If True (default), perform a standard independent 2 sample test\n",
      "        that assumes equal population variances [1]_.\n",
      "        If False, perform Welch's t-test, which does not assume equal\n",
      "        population variance [2]_.\n",
      "    \n",
      "        .. versionadded:: 0.11.0\n",
      "    nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "        Defines how to handle when input contains nan.\n",
      "        The following options are available (default is 'propagate'):\n",
      "    \n",
      "          * 'propagate': returns nan\n",
      "          * 'raise': throws an error\n",
      "          * 'omit': performs the calculations ignoring nan values\n",
      "    alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "        Defines the alternative hypothesis.\n",
      "        The following options are available (default is 'two-sided'):\n",
      "    \n",
      "          * 'two-sided'\n",
      "          * 'less': one-sided\n",
      "          * 'greater': one-sided\n",
      "    \n",
      "        .. versionadded:: 1.6.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    statistic : float or array\n",
      "        The calculated t-statistic.\n",
      "    pvalue : float or array\n",
      "        The two-tailed p-value.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    We can use this test, if we observe two independent samples from\n",
      "    the same or different population, e.g. exam scores of boys and\n",
      "    girls or of two ethnic groups. The test measures whether the\n",
      "    average (expected) value differs significantly across samples. If\n",
      "    we observe a large p-value, for example larger than 0.05 or 0.1,\n",
      "    then we cannot reject the null hypothesis of identical average scores.\n",
      "    If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%,\n",
      "    then we reject the null hypothesis of equal averages.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n",
      "    \n",
      "    .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy import stats\n",
      "    >>> np.random.seed(12345678)\n",
      "    \n",
      "    Test with sample with identical means:\n",
      "    \n",
      "    >>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
      "    >>> rvs2 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
      "    >>> stats.ttest_ind(rvs1,rvs2)\n",
      "    (0.26833823296239279, 0.78849443369564776)\n",
      "    >>> stats.ttest_ind(rvs1,rvs2, equal_var = False)\n",
      "    (0.26833823296239279, 0.78849452749500748)\n",
      "    \n",
      "    `ttest_ind` underestimates p for unequal variances:\n",
      "    \n",
      "    >>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500)\n",
      "    >>> stats.ttest_ind(rvs1, rvs3)\n",
      "    (-0.46580283298287162, 0.64145827413436174)\n",
      "    >>> stats.ttest_ind(rvs1, rvs3, equal_var = False)\n",
      "    (-0.46580283298287162, 0.64149646246569292)\n",
      "    \n",
      "    When n1 != n2, the equal variance t-statistic is no longer equal to the\n",
      "    unequal variance t-statistic:\n",
      "    \n",
      "    >>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100)\n",
      "    >>> stats.ttest_ind(rvs1, rvs4)\n",
      "    (-0.99882539442782481, 0.3182832709103896)\n",
      "    >>> stats.ttest_ind(rvs1, rvs4, equal_var = False)\n",
      "    (-0.69712570584654099, 0.48716927725402048)\n",
      "    \n",
      "    T-test with different means, variance, and n:\n",
      "    \n",
      "    >>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100)\n",
      "    >>> stats.ttest_ind(rvs1, rvs5)\n",
      "    (-1.4679669854490653, 0.14263895620529152)\n",
      "    >>> stats.ttest_ind(rvs1, rvs5, equal_var = False)\n",
      "    (-0.94365973617132992, 0.34744170334794122)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "help(ttest_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-11.893299030876712, pvalue=4.461230231620717e-31)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the tstat and value again but this time using scipy\n",
    "\n",
    "charges_i = xs.charges.loc[xs['insuranceclaim'] == 1]\n",
    "charges_u = xs.charges.loc[xs['insuranceclaim'] != 1]\n",
    "ttest_ind(charges_u, charges_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manual method did match the scipy method. \n",
    "Because the p is very small, we can reject the alternate hypothesis that the means are the same and we can not reject our null hypothesis that the means are different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Hopefully you got the exact same numerical results. This shows that you correctly calculated the numbers by hand. Secondly, you used the correct function and saw that it's much easier to use. All you need to do is pass your data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q6:__ Conceptual question: look through the documentation for statistical test functions in scipy.stats. You'll see the above _t_-test for a sample, but can you see an equivalent one for performing a *z*-test from a sample? Comment on your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ There is no z-test listed [here](https://docs.scipy.org/doc/scipy/reference/stats.html#statistical-tests)\n",
    "With a large enough sample size, the t-test should be the same as the z-test anyway. If the sample is not large, then a z-test is no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed this project notebook, you now have good hands-on experience:\n",
    "* using the central limit theorem to help you apply frequentist techniques to answer questions that pertain to very non-normally distributed data from the real world\n",
    "* performing inference using such data to answer business questions\n",
    "* forming a hypothesis and framing the null and alternative hypotheses\n",
    "* testing this using a _t_-test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
